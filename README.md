# mlprojects
This project is a slight variation on the Cats VS Dogs code found at:
https://ahmedbesbes.com/understanding-deep-convolutional-neural-networks-with-a-practical-use-case-in-tensorflow-and-keras.html
The purpose of this was to create a model that would be able to achieve a reasonable degree fo accuracy. My personal goal for this project is somewhere between 50-75% accuracy, based on limitations listed below. Later, this project will be re-evaluated in an attempt to achieve a higher level of accuracy, hopefully in the 85%+ range with hopefully a more optimized computing method (see limitations).

A convolutional neural network was used because it is one of the staple types of image recogination in machine learning.  The chosen CNN was used because it seemed to provide good results with the Cats VS Dogs dataset and my hope was that it would be abole to produce similiar results.  With the size of the dataset, there is roughly a 0.83% chance for the model to randomly guess the correct breed.

Some code has been changed to suit my specific project, but the setup of the CNN remains intact (minus output layer adjustments). Some changes were made regarding batch sizing, epoch number, the optimizer and the removal of the auto stop feature. This feature seemed to cause the project to always end early.

The dataset is from the Stanford Dog Breed Dataset located at http://vision.stanford.edu/aditya86/ImageNetDogs/

The dataset was renamed to be more inline with the structural layout for the project code. Additionally, a random number of validation pictures were taken, around 20% of the total images. This reworked dataset is included.

Thus far, this project has yielded about a 17% validation accuracy. But here are some limiting factors at play. First, bounding boxes were not used in this set up. The original dataset included individual bounding boxes for each image, however for "fun" I decided to see how the model reacted to not having these included. Second, the dataset is quite large, with 120 classifications, this set requires quite a large amount of computation. I do not have a GPU setup at home and decided to run the learning from an AWS VM g3.4xlarge instance. Even though the data is saved locally on the VM the project seems to be streaming the training and validation from my personal computer, limiting the speed at which data is processed. As it stands the VM is at about 65% utilization, which is a major bottleneck. Finally, it is my opinion that the accuracy will not be very high based on the data selection. Breeds like the Golden Retriever and the yellow Labradore Retriever, at least to my eye, are identical. There are several breeds in the dataset that share this similarity. With the implementation of bounding boxes, a greater understanding of AWS the computation time should decrease and the accuracy should increase.

While this project is initially a class project, this project is not fully finished. I plan to return, adjust the model, and remove some of the limitations and compare the output. Items to compare are, accuracy vs computational time and the overall accuracy achieved.  Most of this work will be complete when I have full time access to a GPU based computer, since over time the cost of AWS will eventually surpass that of purchasing a high quality graphics card.
